{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance: Comparing Different Models\n",
    "\n",
    "In our `training.ipynb` notebook, we built a Decision Tree model that achieved 93% accuracy. In this notebook, we will build on that work by testing several other powerful models to see if we can improve upon that result. We will use the exact same training and testing data to ensure a fair comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Data Loading\n",
    "\n",
    "First, we repeat the same initial steps: import libraries, load the data, and split it into the same training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Import the models we want to test\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load and prepare the data\n",
    "df = pd.read_csv('../data/iris.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Use the same random_state to get the exact same split as before\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dictionary of Models\n",
    "\n",
    "To make our comparison clean and easy, we will store our models in a dictionary. This allows us to loop through them and run the same evaluation steps for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate Each Model\n",
    "\n",
    "Now, we will iterate through our dictionary. For each model, we will:\n",
    "1. Train it on the training data.\n",
    "2. Make predictions on the test data.\n",
    "3. Print a classification report to see its performance in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Decision Tree ---\n",
      "Accuracy: 0.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.90      0.90      0.90        10\n",
      "           2       0.90      0.90      0.90        10\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.93      0.93      0.93        30\n",
      "weighted avg       0.93      0.93      0.93        30\n",
      "\n",
      "----------------------------------------\n",
      "--- Random Forest ---\n",
      "Accuracy: 0.90\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n",
      "----------------------------------------\n",
      "--- Support Vector Machine ---\n",
      "Accuracy: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      0.90      0.95        10\n",
      "           2       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "----------------------------------------\n",
      "--- K-Nearest Neighbors ---\n",
      "Accuracy: 1.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "           2       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f'--- {name} ---')\n",
    "    print(f'Accuracy: {accuracy_score(y_test, y_pred):.2f}')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('' + '-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Importance\n",
    "\n",
    "The Random Forest model can provide us with \"feature importances,\" which tell us which features the model found most useful for making its predictions. Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances from the trained Random Forest model\n",
    "importances = models['Random Forest'].feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Plot the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** As expected, the model confirms that `petal width` and `petal length` are by far the most important features for classifying the flowers. This aligns perfectly with our findings from the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/iris_K-Nearest_Neighbors.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model = models['K-Nearest Neighbors']  # Choose the KNN model for saving\n",
    "\n",
    "# Define the filename for the model\n",
    "model_filename = '../models/iris_K-Nearest_Neighbors.joblib'\n",
    "\n",
    "# Save the model to the file\n",
    "joblib.dump(model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Conclusion: Analyzing the Results\n",
    "\n",
    "\n",
    "#### K-Nearest Neighbors (KNN): 100% Accuracy\n",
    "- **How it Works:** KNN is a simple but powerful model. It classifies a new flower by looking at the 'k' closest flowers in the training data and taking a majority vote. For the Iris dataset, the species are so well-clustered that a new flower is almost always surrounded by neighbors of its own kind. \n",
    "- **Why it Performed Best:** It achieved a perfect score because the feature space of the Iris dataset is very well-defined. The clusters of species are dense and clearly separated, making it easy for KNN to find the correct neighbors and make the right prediction every time.\n",
    "\n",
    "#### Support Vector Machine (SVM): 97% Accuracy\n",
    "- **How it Works:** An SVM works by finding the optimal hyperplane or boundary that best separates the classes. It tries to maximize the margin (the distance) between the different classes.\n",
    "- **Why it Performed Well:** SVMs are excellent at finding the subtle, non-linear boundaries that might exist between classes. It achieved 97% accuracy, only making one mistake between species 1 and 2. This shows it was very effective at defining the decision boundaries, even for the two more similar species.\n",
    "\n",
    "#### Decision Tree: 93% Accuracy\n",
    "- **How it Works:** A Decision Tree makes predictions by creating a set of if-then-else rules based on the features. It splits the data at each node to create the purest possible child nodes.\n",
    "- **Why it Performed Well (but not perfectly):** Our original model still performed very well. Its 93% accuracy shows that simple, interpretable rules are enough to solve most of this problem. Its errors occurred where the boundary between species 1 and 2 is a bit fuzzy, which can sometimes challenge a single tree.\n",
    "\n",
    "#### Random Forest: 90% Accuracy\n",
    "- **How it Works:** A Random Forest is an ensemble of many Decision Trees. It builds multiple trees on different subsets of the data and features, and the final prediction is a majority vote from all the trees.\n",
    "- **Why it Underperformed (in this specific case):** It is surprising that the Random Forest performed worse than the single Decision Tree. While usually more robust, a Random Forest introduces randomness in its feature selection for each tree. For a simple dataset like Iris where only a couple of features are critical (like petal width and length), this randomness might have occasionally caused some trees to be built without access to the most important features, leading to a few incorrect votes in the final ensemble. This is a rare case and a good learning example that more complex models are not *always* better on simpler datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
