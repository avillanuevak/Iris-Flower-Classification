{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Advanced Topic - Hyperparameter Tuning\n",
    "\n",
    "In our `04_Model_Comparison.ipynb` notebook, we found that several models could achieve high accuracy on the Iris dataset. However, for more complex, real-world problems, getting the best performance requires **hyperparameter tuning**. This notebook explains what that means and how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Hyperparameters?\n",
    "\n",
    "Think of hyperparameters as the settings or knobs for a machine learning model. We can tune these settings to find the combination that yields the best performance. For this example, we will use a **Random Forest** model, which we saw performed not as well as the others due to its inherent randomness.\n",
    "\n",
    "Some key hyperparameters for a Random Forest include:\n",
    "- `n_estimators`: The number of decision trees in the forest.\n",
    "- `max_depth`: The maximum depth of each tree.\n",
    "- `min_samples_leaf`: The minimum number of samples required to be at a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load and prepare the data\n",
    "df = pd.read_csv('../data/iris.csv')\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Use the same random_state to get the exact same split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Hyperparameter Grid\n",
    "\n",
    "We will create a \"grid\" of hyperparameters to test. `GridSearchCV` will then systematically try every combination of these settings to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],      # Test with 50, 100, or 200 trees\n",
    "    'max_depth': [None, 10, 20, 30],       # Test different tree depths\n",
    "    'min_samples_leaf': [1, 2, 4]         # Test different minimum leaf sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Perform the Grid Search\n",
    "\n",
    "Now we create an instance of `GridSearchCV`. We pass it our model, the parameter grid, and tell it to use `cv=5` (5-fold cross-validation), which is a standard practice to ensure the results are robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Grid search complete!\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data (this will take a moment)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Grid search complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the Best Parameters and Evaluate\n",
    "\n",
    "After the search is complete, we can see which combination of hyperparameters performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'min_samples_leaf': 1, 'n_estimators': 50}\n",
      "--- Final Report on Best Model ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "           2       0.89      0.80      0.84        10\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.90      0.90      0.90        30\n",
      "weighted avg       0.90      0.90      0.90        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Get the best model found by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Print the final classification report\n",
    "print('--- Final Report on Best Model ---')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy of the Tuned Model: 90.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Final Accuracy of the Tuned Model: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/iris_random_forest_best.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving model with best parameters\n",
    "import joblib\n",
    "model_filename = '../models/iris_random_forest_best.joblib'\n",
    "joblib.dump(best_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: When Tuning Doesn't Improve Performance\n",
    "\n",
    "This is a fascinating and important result. In our `04_Model_Comparison.ipynb` notebook, the default Random Forest model scored 90% accuracy. After an extensive hyperparameter search, the final accuracy of our tuned model remained at **90%**. This demonstrates a critical concept in machine learning: **tuning is not a guaranteed improvement.**\n",
    "\n",
    "**Why didn't the accuracy improve?**\n",
    "1.  **Small Dataset:** The Iris dataset is very small. With only 30 samples in our test set, the default parameters of the Random Forest were likely already sufficient to learn the patterns effectively. There isn't enough data to find a significantly more optimal set of hyperparameters.\n",
    "2.  **Inherent Data Overlap:** The errors in the Random Forest model occurred when distinguishing between species 1 (Versicolor) and 2 (Virginica). It's likely that the few data points that were misclassified are genuinely ambiguous and lie in an overlapping region of the feature space. No amount of tuning the model's settings can fix ambiguity that is inherent to the data itself.\n",
    "\n",
    "#### Is it Worth Tuning a Different Model?\n",
    "\n",
    "Let's look at our results from the `04_Model_Comparison.ipynb` notebook:\n",
    "- **K-Nearest Neighbors (KNN): 100% Accuracy**\n",
    "- **Support Vector Machine (SVM): 97% Accuracy**\n",
    "\n",
    "Given that the default KNN model already achieved a perfect score, there is no practical benefit to be gained from tuning it further for this specific problem. The goal is to find the best possible model, and we have already achieved that with a simpler approach.\n",
    "\n",
    "**Final Takeaway:** The most valuable step in this exercise was the **model comparison**. It quickly showed us that for the Iris dataset, the KNN algorithm is a superior choice to the Random Forest. This teaches us an important lesson: it's often more effective to try several different types of models than it is to spend a great deal of time tuning a single model that may not be the best fit for the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
